from opensearchpy import OpenSearch
from typing import List, Dict, Any, Optional
import sys
import os

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from file.search import create_opensearch_client, search_chunks, vector_search_chunks, hybrid_search_chunks
from file.upstage import create_embeddings_client
from langchain_openai import AzureChatOpenAI
import os
from dotenv import load_dotenv

load_dotenv()


def rag_search(
    query: str,
    client: Optional[OpenSearch] = None,
    search_type: str = "hybrid",
    size: int = 5
) -> Dict[str, Any]:
    """
    OpenSearchë¥¼ ì‚¬ìš©í•œ RAG ê¸°ë°˜ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬
        client: OpenSearch í´ë¼ì´ì–¸íŠ¸ (Noneì´ë©´ ìƒˆë¡œ ìƒì„±)
        search_type: ê²€ìƒ‰ íƒ€ì… ("text", "vector", "hybrid")
        size: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
        
    Returns:
        ê²€ìƒ‰ ê²°ê³¼ì™€ ê´€ë ¨ ë©”íƒ€ë°ì´í„°
    """
    if client is None:
        client = create_opensearch_client()
    
    # ê²€ìƒ‰ ìˆ˜í–‰
    search_results = []
    
    if search_type == "text":
        search_results = search_chunks(client, query, size=size)
    elif search_type == "vector":
        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        embeddings_client = create_embeddings_client()
        if embeddings_client:
            try:
                query_embedding = embeddings_client.embed_query(query)
                search_results = vector_search_chunks(client, query_embedding, size=size)
            except Exception as e:
                print(f"ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨, í…ìŠ¤íŠ¸ ê²€ìƒ‰ìœ¼ë¡œ ëŒ€ì²´: {e}")
                search_results = search_chunks(client, query, size=size)
        else:
            search_results = search_chunks(client, query, size=size)
    elif search_type == "hybrid":
        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì„ ìœ„í•œ ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        embeddings_client = create_embeddings_client()
        query_vector = None
        if embeddings_client:
            try:
                query_vector = embeddings_client.embed_query(query)
            except Exception as e:
                print(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨, í…ìŠ¤íŠ¸ ê²€ìƒ‰ë§Œ ì‚¬ìš©: {e}")
        
        search_results = hybrid_search_chunks(client, query, query_vector, size=size)
    
    return {
        "query": query,
        "search_results": search_results,
        "total_results": len(search_results)
    }

def get_context_from_results(results: List[Dict[str, Any]], max_context_length: int = 50000) -> str:
    """
    ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    
    Args:
        results: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        max_context_length: ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´
        
    Returns:
        ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´
    """
    context_parts = []
    current_length = 0
    
    for result in results:
        content = result.get("content", "")
        source = f"[ì¶œì²˜: {result.get('document_name', 'unknown')}]"
        
        # ë‹¨ì¼ ì²­í¬ê°€ max_context_lengthë¥¼ ì´ˆê³¼í•˜ëŠ” ê²½ìš° ì˜ë¼ì„œ í¬í•¨
        if len(content) > max_context_length:
            content = content[:max_context_length-len(source)-10] + "..."
        
        formatted_content = f"{source}\n{content}"
        
        if current_length + len(formatted_content) > max_context_length:
            # ë‚¨ì€ ê³µê°„ì´ ìˆìœ¼ë©´ ë¶€ë¶„ì ìœ¼ë¡œë¼ë„ í¬í•¨
            remaining_space = max_context_length - current_length
            if remaining_space > len(source) + 50:  # ìµœì†Œí•œì˜ ë‚´ìš©ì´ ë“¤ì–´ê°ˆ ê³µê°„ì´ ìˆìœ¼ë©´
                truncated_content = content[:remaining_space-len(source)-10] + "..."
                context_parts.append(f"{source}\n{truncated_content}")
            break
        
        context_parts.append(formatted_content)
        current_length += len(formatted_content)
    
    return "\n\n".join(context_parts)

def rag_query(
    question: str,
    client: Optional[OpenSearch] = None,
    search_type: str = "hybrid",
    context_size: int = 5,
    max_context_length: int = 50000
) -> Dict[str, Any]:
    """
    RAGë¥¼ ì‚¬ìš©í•œ ì§ˆì˜ì‘ë‹µì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    Args:
        question: ì§ˆë¬¸
        client: OpenSearch í´ë¼ì´ì–¸íŠ¸
        search_type: ê²€ìƒ‰ íƒ€ì…
        context_size: ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©í•  ê²€ìƒ‰ ê²°ê³¼ ìˆ˜
        max_context_length: ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´
        
    Returns:
        ì§ˆë¬¸, ì»¨í…ìŠ¤íŠ¸, ê²€ìƒ‰ ë©”íƒ€ë°ì´í„°ë¥¼ í¬í•¨í•œ ë”•ì…”ë„ˆë¦¬
    """
    # RAG ê²€ìƒ‰ ìˆ˜í–‰
    rag_result = rag_search(
        query=question,
        client=client,
        search_type=search_type,
        size=context_size
    )
    
    # ì»¨í…ìŠ¤íŠ¸ ìƒì„±
    context = get_context_from_results(
        rag_result["search_results"],
        max_context_length=max_context_length
    )
    
    return {
        "question": question,
        "context": context,
        "search_metadata": {
            "total_results": rag_result["total_results"]
        },
        "search_results": rag_result["search_results"]
    }

def create_llm_client():
    """Azure OpenAI LLM í´ë¼ì´ì–¸íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        llm = AzureChatOpenAI(
            azure_deployment=os.getenv("AZURE_OPENAI_DEPLOYMENT"),  # AZURE_OPENAI_DEPLOYMENT ì‚¬ìš©
            azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
            api_key=os.getenv("AZURE_OPENAI_API_KEY"),
            api_version=os.getenv("AZURE_OPENAI_API_VERSION", "2024-02-01"),
            temperature=0.1
        )
        return llm
    except Exception as e:
        print(f"âš ï¸ LLM í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
        return None

def generate_answer_with_llm(question: str, context: str, llm_client=None) -> str:
    """
    ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ LLMì„ ì‚¬ìš©í•´ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        question: ì§ˆë¬¸
        context: ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸
        llm_client: LLM í´ë¼ì´ì–¸íŠ¸ (Noneì´ë©´ ìƒˆë¡œ ìƒì„±)
        
    Returns:
        ìƒì„±ëœ ë‹µë³€
    """
    if not context:
        return "ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    if llm_client is None:
        llm_client = create_llm_client()
        if llm_client is None:
            return "LLM ì„œë¹„ìŠ¤ì— ì—°ê²°í•  ìˆ˜ ì—†ì–´ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    prompt = f"""Please provide an answer to the question based on the following context.

Context:
{context}

Question: {question}

Please follow this format when answering:
1. First, quote the exact relevant text from the context in its original language (English)
2. Then provide a Korean translation/explanation of that content
3. Structure your answer like this:
   - Start with the exact quote from the document
   - Then add: "ì£¼ì–´ì§„ ë¬¸ì„œì—ì„œ [topic]ì— ëŒ€í•œ ì •ë³´ëŠ” ìœ„ì™€ ê°™ìŠµë‹ˆë‹¤."
   - Follow with a Korean explanation/translation
4. write page number in the context
Guidelines:
- Use only the information contained in the context
- Quote the exact text that answers the question
- Provide accurate Korean translation
- If no relevant information is found, respond with "ì œê³µëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"

Answer:"""

    try:
        response = llm_client.invoke(prompt)
        return response.content.strip()
    except Exception as e:
        print(f"âš ï¸ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def main():
    """RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    client = create_opensearch_client()
    llm_client = create_llm_client()
    
    # ìƒ˜í”Œ ì§ˆë¬¸ë“¤
    sample_questions = [
        # "What type of gate valve is required for ASME class 1500 and higher according to the specification?",
        # "Are plastic valve handwheels acceptable under this specification?",
        # "According to the 'General' section, what is the primary responsibility of the Supplier regarding their equipment and accessories, and what specific aspects should their equipment incorporate?",
        # "What is the minimum operational history in an industrial plant required for the equipment offered by the Supplier?",
        # "Before initiating fabrication work, what crucial approval must the Supplier obtain, and what must be completed at the factory prior to shipment?",
        "FORGING",
        "about CASTING",
        "Proposal Requirements"
    ]
    
    for question in sample_questions:
        print(f"\nì§ˆë¬¸: {question}")
        print("-" * 80)
        
        result = rag_query(question, client)
        
        if result['context']:
            # LLMì„ ì‚¬ìš©í•´ ë‹µë³€ ìƒì„±
            answer = generate_answer_with_llm(question, result['context'], llm_client)
            print(f"\nğŸ’¬ AI ë‹µë³€:\n{answer}")
            
            # ì°¸ê³ í•œ ì²­í¬ ì •ë³´ í‘œì‹œ (ì»¨í…ìŠ¤íŠ¸ì— ì‹¤ì œ í¬í•¨ëœ ì²­í¬ë“¤)
            chunk_ids = [str(res.get('chunk_id', 'unknown')) for res in result['search_results'] if res.get('content')]
            print(f"\nğŸ“„ ì°¸ê³ í•œ ì²­í¬: {', '.join(chunk_ids[:5])}")  # ìƒìœ„ 5ê°œ í‘œì‹œ
            
            # ë””ë²„ê¹… ì •ë³´ (ì„ íƒì )
            print(f"\nğŸ“Š ê²€ìƒ‰ ë©”íƒ€ë°ì´í„°:")
            print(f"  - ì´ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜: {result['search_metadata']['total_results']}")
            
        else:
            print("\nâŒ ê´€ë ¨ëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()